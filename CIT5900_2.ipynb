{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n1xd/5900/blob/main/CIT5900_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9t2s5UW8GkzR"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "!pip install requests beautifulsoup4 lxml pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP_mN_1J-3p4"
      },
      "source": [
        "# Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YNfsCjA-5yz"
      },
      "outputs": [],
      "source": [
        "# PI, year, title, DOI/unique identifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYpZMGMF-yX3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from urllib.parse import quote_plus, unquote\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0eJ8nSYZcsH"
      },
      "outputs": [],
      "source": [
        "# load PIs from excel file (helper)\n",
        "def load_excel_data(file_path, sheet_name=\"All Metadata\"):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "        print(f\"Loaded {len(df)} records from Excel\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Excel file: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L5MhTFKZ4fi"
      },
      "outputs": [],
      "source": [
        "# load PIs (run)\n",
        "excel_path = \"projects_all_metadata.xlsx\"\n",
        "df = load_excel_data(excel_path)\n",
        "\n",
        "# show selected\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "print(df[['Proj ID', 'Title', 'Start Year', 'End Year', 'PI']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZxNKf2XaibL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "\n",
        "# load metadata authors\n",
        "authors = df['PI'].dropna().unique().tolist()  # limiting to 100 for testing\n",
        "\n",
        "# define keywords - convert to lowercase for case-insensitive matching\n",
        "fsrdc_keywords = [\n",
        "    'fsrdc', 'census bureau', 'restricted data', 'restricted microdata',\n",
        "    'disclosure review', 'confidentiality review',\n",
        "    'annual survey of manufactures', 'census of manufactures',\n",
        "    'michigan rdc', 'irs', 'bea', 'federal statistical'\n",
        "]\n",
        "fsrdc_keywords = [keyword.lower() for keyword in fsrdc_keywords]\n",
        "\n",
        "# prep list for results (later df)\n",
        "matched_papers = []\n",
        "\n",
        "for i, author in enumerate(authors, start=1):\n",
        "    # Format author name for URL\n",
        "    formatted_author = author.replace(\" \", \"+\")\n",
        "    url = f\"https://arxiv.org/search/?query={formatted_author}&searchtype=author\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        results = soup.find_all('li', class_='arxiv-result')\n",
        "\n",
        "        print(f\"Search {i}/{len(authors)}: {author} - Found {len(results)} papers\")\n",
        "\n",
        "        for r in results[:10]:  # check up to 10 papers per author\n",
        "            try:\n",
        "                title = r.find('p', class_='title').text.strip()\n",
        "                abs_link_element = r.find('p', class_='list-title').find('a')\n",
        "                if abs_link_element and 'href' in abs_link_element.attrs:\n",
        "                    abs_link = abs_link_element['href']\n",
        "                    if not abs_link.startswith('http'):\n",
        "                        abs_link = f\"https://arxiv.org{abs_link}\"\n",
        "\n",
        "                    print(f\"checking paper: {title[:50]}...\")\n",
        "                    print(f\"link: {abs_link}\")\n",
        "\n",
        "                    # parse abstract\n",
        "                    abs_response = requests.get(abs_link)\n",
        "                    abs_soup = BeautifulSoup(abs_response.text, \"html.parser\")\n",
        "                    abstract_element = abs_soup.find('blockquote', class_='abstract')\n",
        "\n",
        "                    if abstract_element:\n",
        "                        # Normalize abstract: remove extra spaces, convert to lowercase\n",
        "                        raw_abstract = abstract_element.text.strip()\n",
        "                        abstract = re.sub(r'Abstract: ', '', raw_abstract, flags=re.IGNORECASE)\n",
        "                        abstract = re.sub(r'\\s+', ' ', abstract).lower()  # Normalize whitespace and lowercase\n",
        "\n",
        "                        print(f\"abstract snippet: {abstract[:100]}...\")\n",
        "\n",
        "                        # Also check the title for keywords\n",
        "                        normalized_title = title.lower()\n",
        "                        combined_text = f\"{normalized_title} {abstract}\"\n",
        "\n",
        "                        # check for keyword match in abstract and title\n",
        "                        found_keywords = []\n",
        "                        for keyword in fsrdc_keywords:\n",
        "                            # For single-word keywords, use word boundaries\n",
        "                            if ' ' not in keyword:\n",
        "                                pattern = rf\"\\b{re.escape(keyword)}\\b\"\n",
        "                                if re.search(pattern, combined_text):\n",
        "                                    found_keywords.append(keyword)\n",
        "                                    print(f\"Found keyword: '{keyword}' as whole word\")\n",
        "                            # For multi-word keywords, try partial word matching but ensure each word is a standalone word\n",
        "                            else:\n",
        "                                words = keyword.split()\n",
        "                                # Check if all words in the multi-word keyword exist as whole words\n",
        "                                if all(re.search(rf\"\\b{re.escape(word)}\\b\", combined_text) for word in words):\n",
        "                                    found_keywords.append(keyword)\n",
        "                                    print(f\"Found multi-word keyword: '{keyword}'\")\n",
        "\n",
        "                        if found_keywords:  # Only process if keywords were found\n",
        "                            print(f\"keywords found: {', '.join(found_keywords)}\")\n",
        "\n",
        "                            # get year + doi (if possible, will filter later)\n",
        "                            history = abs_soup.find('div', class_='submission-history')\n",
        "                            year = None\n",
        "                            if history:\n",
        "                                match = re.search(r'\\b(20\\d{2})\\b', history.text)\n",
        "                                year = match.group(0) if match else None\n",
        "                            else:\n",
        "                                year = None\n",
        "\n",
        "                            doi_element = abs_soup.find('td', class_='tablecell doi')\n",
        "                            doi = doi_element.text.strip() if doi_element else None\n",
        "\n",
        "                            matched_papers.append({\n",
        "                                \"pi\": author,\n",
        "                                \"title\": title,\n",
        "                                \"year\": year,\n",
        "                                \"doi\": doi if doi else \"N/A\",\n",
        "                                \"abstract\": raw_abstract,  # Store original abstract\n",
        "                                \"keywords_found\": \", \".join(found_keywords),\n",
        "                                \"url\": abs_link\n",
        "                            })\n",
        "                        else:\n",
        "                            print(f\"no keywords matched in paper\")\n",
        "                    else:\n",
        "                        print(f\"no abstract\")\n",
        "            except Exception as e:\n",
        "                print(f\"error processing paper: {str(e)}\")\n",
        "\n",
        "        time.sleep(3)  # gentle scraping\n",
        "    except Exception as e:\n",
        "        print(f\"error searching for author {author}: {str(e)}\")\n",
        "\n",
        "# save\n",
        "if matched_papers:\n",
        "    matched_df = pd.DataFrame(matched_papers)\n",
        "    matched_df.to_csv(\"fsrdc_keyword_matches.csv\", index=False)\n",
        "    print(f\"{len(matched_df)} matching papers saved\")\n",
        "else:\n",
        "    print(\"no matches found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br9rTXaPbl9i"
      },
      "outputs": [],
      "source": [
        "print(matched_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnyYXkwoZwgn"
      },
      "source": [
        "# Some API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlqgaFfdp9Qa"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "MAX_RETRIES = 3\n",
        "DELAY_BETWEEN_REQUESTS = 2  # seconds\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmkCRG6yqQtu"
      },
      "outputs": [],
      "source": [
        "# load PIs from excel file (helper)\n",
        "def load_excel_data(file_path, sheet_name=\"All Metadata\"):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "        print(f\"Loaded {len(df)} records from Excel\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Excel file: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKEeBmMxqhHI"
      },
      "outputs": [],
      "source": [
        "# load PIs (run)\n",
        "excel_path = \"projects_all_metadata.xlsx\"\n",
        "df = load_excel_data(excel_path)\n",
        "\n",
        "# show selected\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "print(df[['Proj ID', 'Title', 'Start Year', 'End Year', 'PI']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U4n_XWkq6O4"
      },
      "outputs": [],
      "source": [
        "# API crossref function\n",
        "def search_scholarly_articles(pi_name, limit=10):\n",
        "    \"\"\"Search for scholarly articles by PI name using Crossref API.\"\"\"\n",
        "    base_url = \"https://api.crossref.org/works\"\n",
        "    params = {\n",
        "        \"query.author\": pi_name,\n",
        "        \"rows\": limit,\n",
        "        \"sort\": \"relevance\",\n",
        "        \"order\": \"desc\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results = []\n",
        "\n",
        "            for item in data.get(\"message\", {}).get(\"items\", []):\n",
        "                # Extract metadata\n",
        "                title = item.get(\"title\", [\"\"])[0] if item.get(\"title\") else \"\"\n",
        "\n",
        "                # Extract authors\n",
        "                authors = []\n",
        "                for author in item.get(\"author\", []):\n",
        "                    if \"given\" in author and \"family\" in author:\n",
        "                        authors.append(f\"{author['given']} {author['family']}\")\n",
        "\n",
        "                # Extract year\n",
        "                year = None\n",
        "                if \"published-print\" in item and \"date-parts\" in item[\"published-print\"]:\n",
        "                    year = item[\"published-print\"][\"date-parts\"][0][0]\n",
        "                elif \"published-online\" in item and \"date-parts\" in item[\"published-online\"]:\n",
        "                    year = item[\"published-online\"][\"date-parts\"][0][0]\n",
        "\n",
        "                # Extract DOI\n",
        "                doi = item.get(\"DOI\", \"\")\n",
        "\n",
        "                # Extract URL\n",
        "                url = f\"https://doi.org/{doi}\" if doi else \"\"\n",
        "\n",
        "                results.append({\n",
        "                    \"title\": title,\n",
        "                    \"authors\": authors,\n",
        "                    \"year\": year,\n",
        "                    \"doi\": doi,\n",
        "                    \"url\": url\n",
        "                })\n",
        "\n",
        "            return results\n",
        "        else:\n",
        "            print(f\"Error searching Crossref API: Status code {response.status_code}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Exception when searching scholarly articles: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viQmfkuUrCWt"
      },
      "outputs": [],
      "source": [
        "# test API cross\n",
        "if df is not None and len(df) > 0:\n",
        "    sample_pi = df['PI'].iloc[0]\n",
        "    print(f\"Testing Crossref API with sample PI: {sample_pi}\")\n",
        "    sample_results = search_scholarly_articles(sample_pi)\n",
        "    print(f\"Found {len(sample_results)} papers for {sample_pi}\")\n",
        "    if sample_results:\n",
        "        for i, paper in enumerate(sample_results):  # Show first 3 results\n",
        "            print(f\"\\nPaper {i+1}:\")\n",
        "            print(f\"Title: {paper['title']}\")\n",
        "            print(f\"Authors: {', '.join(paper['authors'])}\")\n",
        "            print(f\"Year: {paper['year']}\")\n",
        "            print(f\"DOI: {paper['doi']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oEvzwY0uA3N"
      },
      "outputs": [],
      "source": [
        "fsrdc_keywords = [\n",
        "    'fsrdc', 'census bureau', 'restricted data', 'restricted microdata',\n",
        "    'disclosure review', 'confidentiality review',\n",
        "    'annual survey of manufactures', 'census of manufactures',\n",
        "    'michigan rdc', 'irs', 'bea'\n",
        "]\n",
        "\n",
        "def fetch_abstract_from_doi(doi):\n",
        "    if not doi:\n",
        "        return None\n",
        "\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            item = response.json().get(\"message\", {})\n",
        "            abstract = item.get(\"abstract\", None)\n",
        "            if abstract:\n",
        "                # Strip HTML if present\n",
        "                return BeautifulSoup(abstract, \"html.parser\").get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching abstract for DOI {doi}: {e}\")\n",
        "    return None\n",
        "\n",
        "def score_fsrdc_in_abstract(abstract, keywords=fsrdc_keywords):\n",
        "    if not abstract:\n",
        "        return 0\n",
        "    text = abstract.lower()\n",
        "    return sum(1 for kw in keywords if kw in text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbazrALYuFxK"
      },
      "outputs": [],
      "source": [
        "def process_pis_for_papers(df, limit_pis=200):\n",
        "    all_papers = []\n",
        "    unique_pis = df['PI'].dropna().unique()[:limit_pis]\n",
        "\n",
        "    print(f\"🔍 Searching papers for {len(unique_pis)} PIs...\")\n",
        "\n",
        "    for i, pi_name in enumerate(unique_pis):\n",
        "        print(f\"\\nPI {i+1}/{len(unique_pis)}: {pi_name}\")\n",
        "        papers = search_scholarly_articles(pi_name, limit=5)\n",
        "\n",
        "        for paper in papers:\n",
        "            paper['pi'] = pi_name\n",
        "            abstract = fetch_abstract_from_doi(paper['doi'])\n",
        "            paper['abstract'] = abstract\n",
        "            paper['fsrdc_score'] = score_fsrdc_in_abstract(abstract)\n",
        "\n",
        "        all_papers.extend(papers)\n",
        "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "\n",
        "    return pd.DataFrame(all_papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCx8Z1PhuLPW"
      },
      "outputs": [],
      "source": [
        "# see sample\n",
        "sample_df = process_pis_for_papers(df)\n",
        "\n",
        "sorted_df = sample_df.sort_values(by='fsrdc_score', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_rtxjCBuT2x"
      },
      "outputs": [],
      "source": [
        "fsrdc_related = sample_df[sample_df['fsrdc_score'] > 0]\n",
        "cleaned_df = fsrdc_related.dropna(subset=['title', 'authors', 'year']).copy() # drop rows that miss year\n",
        "cleaned_df['year'] = cleaned_df['year'].astype(int) # convert years to int\n",
        "cleaned_df['title'] = cleaned_df['title'].str.lower() # lowercase titles\n",
        "cleaned_df = cleaned_df[cleaned_df['year'] != 2024] # get rid of 2024 papers\n",
        "cleaned_df = cleaned_df.sort_values(by='fsrdc_score', ascending=False) # sort by score\n",
        "\n",
        "# Show the result\n",
        "print(len(cleaned_df), \"papers found\")\n",
        "print(cleaned_df[['title', 'authors', 'year', 'doi', 'fsrdc_score']].head(10))\n",
        "cleaned_df.to_csv(\"fsrdc_related_papers.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TUuwxCp_Enm"
      },
      "source": [
        "# More API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYONLa_FF1ly"
      },
      "source": [
        "Extracting PIs from the excel data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvwiECztmzw9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_unique_pis_from_excel(file_path, sheet_name, possible_pi_columns=['PI', 'Principal Investigator', 'PI Name']):\n",
        "    \"\"\"\n",
        "    Extract all unique Principal Investigators (PIs) from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Excel file\n",
        "        sheet_name (str/int): Name or index of sheet to read\n",
        "        possible_pi_columns (list): Possible column names that might contain PI information\n",
        "\n",
        "    Returns:\n",
        "        list: List of unique PI names\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the Excel file\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "        # Find which column contains PI information\n",
        "        pi_column = None\n",
        "        for col in possible_pi_columns:\n",
        "            if col in df.columns:\n",
        "                pi_column = col\n",
        "                break\n",
        "\n",
        "        if not pi_column:\n",
        "            available_columns = df.columns.tolist()\n",
        "            raise ValueError(\n",
        "                f\"Could not find PI column. Tried: {possible_pi_columns}\\n\"\n",
        "                f\"Available columns: {available_columns}\"\n",
        "            )\n",
        "\n",
        "        # Extract unique PIs with robust cleaning\n",
        "        unique_pis = (\n",
        "            df[pi_column]\n",
        "            .dropna()  # Remove NaN values\n",
        "            .astype(str)  # Convert to string\n",
        "            .str.strip()  # Remove whitespace\n",
        "            .replace('', pd.NA)  # Treat empty strings as NA\n",
        "            .dropna()  # Remove NA values again\n",
        "            .unique()  # Get unique values\n",
        "            .tolist()  # Convert to list\n",
        "        )\n",
        "\n",
        "        print(f\"Found {len(unique_pis)} unique PIs in column '{pi_column}'\")\n",
        "        return unique_pis\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the Excel file: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to your Excel file\n",
        "    excel_file_path = \"ProjectsAllMetadata.xlsx\"\n",
        "\n",
        "    # Get all unique PIs from Sheet 2 only\n",
        "    print(\"Extracting PIs from Sheet 2 only...\")\n",
        "    unique_pis = extract_unique_pis_from_excel(excel_file_path, sheet_name=1)  # Sheet 2 is index 1\n",
        "\n",
        "    if unique_pis:\n",
        "        # Create a DataFrame with the PIs\n",
        "        pi_df = pd.DataFrame({'Principal Investigator': unique_pis})\n",
        "\n",
        "        # Save to CSV\n",
        "        output_file = \"unique_PIs.csv\"\n",
        "        pi_df.to_csv(output_file, index=False)\n",
        "        print(f\"\\nSaved {len(unique_pis)} unique PIs to {output_file}\")\n",
        "\n",
        "        # Print first 10 as a sample\n",
        "        print(\"\\nFirst 10 PIs found:\")\n",
        "        for pi in unique_pis[:10]:\n",
        "            print(pi)\n",
        "    else:\n",
        "        print(\"No PIs found in Sheet 2 of the Excel file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojOhH3HcTX0i"
      },
      "source": [
        "Orcid API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyZHxUR-ubw6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "\n",
        "# ORCID API endpoints\n",
        "ORCID_SEARCH_URL = \"https://pub.orcid.org/v3.0/search/\"\n",
        "ORCID_WORKS_URL = \"https://pub.orcid.org/v3.0/{}/works\"\n",
        "\n",
        "# Headers for ORCID API\n",
        "headers = {\n",
        "    'Accept': 'application/json',\n",
        "}\n",
        "\n",
        "# Criteria for filtering research papers\n",
        "fsrdc_keywords = [\n",
        "    \"Census Bureau\", \"FSRDC\", \"Federal Statistical Research Data Center\",\n",
        "    \"restricted microdata\", \"IRS\", \"BEA\", \"confidentiality review\",\n",
        "    \"Michigan RDC\", \"Texas RDC\", \"Boston RDC\",\n",
        "    \"Annual Survey of Manufactures\", \"Census of Construction Industries\",\n",
        "    \"Census of Finance, Insurance, and Real Estate\"\n",
        "]\n",
        "\n",
        "fsrdc_keywords = [kw.lower() for kw in fsrdc_keywords]  # Convert to lowercase\n",
        "\n",
        "def contains_fsrdc_keyword(text):\n",
        "    \"\"\"Check if text contains any FSRDC keyword\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return False\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in fsrdc_keywords)\n",
        "\n",
        "def search_orcid_by_name(name):\n",
        "    \"\"\"Search ORCID for profiles matching a given name\"\"\"\n",
        "    try:\n",
        "        if not name or not isinstance(name, str):\n",
        "            return None\n",
        "\n",
        "        name_parts = name.strip().split()\n",
        "        if not name_parts:\n",
        "            return None\n",
        "\n",
        "        given_name = name_parts[0]\n",
        "        family_name = name_parts[-1] if len(name_parts) > 1 else \"\"\n",
        "\n",
        "        query = f\"given-names:{given_name}\"\n",
        "        if family_name:\n",
        "            query += f\" AND family-name:{family_name}\"\n",
        "\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'start': 0,\n",
        "            'rows': 1\n",
        "        }\n",
        "\n",
        "        response = requests.get(ORCID_SEARCH_URL, headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if isinstance(data, dict) and data.get('result'):\n",
        "                if isinstance(data['result'], list) and len(data['result']) > 0:\n",
        "                    if isinstance(data['result'][0], dict):\n",
        "                        return data['result'][0].get('orcid-identifier', {}).get('path')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Search error for {name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_orcid_works(orcid_id):\n",
        "    \"\"\"Fetch works for a given ORCID ID\"\"\"\n",
        "    if not orcid_id:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = requests.get(ORCID_WORKS_URL.format(orcid_id), headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Works error for {orcid_id}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_works(pi_name, orcid_id, works_data):\n",
        "    \"\"\"Extract publication data from works, filtering for FSRDC keywords\"\"\"\n",
        "    records = []\n",
        "    if not isinstance(works_data, dict):\n",
        "        return records\n",
        "\n",
        "    try:\n",
        "        for group in works_data.get('group', []):\n",
        "            for work in group.get('work-summary', []):\n",
        "                # Get title\n",
        "                title = work.get('title', {}).get('title', {}).get('value', 'No title')\n",
        "\n",
        "                # Skip if title doesn't contain FSRDC keywords\n",
        "                if not contains_fsrdc_keyword(title):\n",
        "                    continue\n",
        "\n",
        "                # Get year\n",
        "                year = work.get('publication-date', {}).get('year', {}).get('value', '')\n",
        "\n",
        "                # Get DOI\n",
        "                doi = ''\n",
        "                for ext_id in work.get('external-ids', {}).get('external-id', []):\n",
        "                    if ext_id.get('external-id-type', '').lower() == 'doi':\n",
        "                        doi = ext_id.get('external-id-value', '')\n",
        "                        break\n",
        "\n",
        "                # Get journal or venue name if available\n",
        "                journal = ''\n",
        "                if 'journal-title' in work:\n",
        "                    journal = work.get('journal-title', {}).get('value', '')\n",
        "\n",
        "                records.append({\n",
        "                    'PI': pi_name,\n",
        "                    'ORCID_ID': orcid_id,\n",
        "                    'year': year,\n",
        "                    'title': title,\n",
        "                    'DOI': doi,\n",
        "                    'journal': journal\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"Processing error for {pi_name}: {str(e)}\")\n",
        "\n",
        "    return records\n",
        "\n",
        "def main():\n",
        "    # File paths\n",
        "    input_file = 'unique_pis.csv'\n",
        "    output_file = 'orcid_api_output.csv'  # Changed output filename\n",
        "\n",
        "    # Read PI names\n",
        "    try:\n",
        "        with open(input_file, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader, None)  # Skip header\n",
        "            pi_names = [row[0].strip() for row in reader if row and row[0].strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Process all PIs\n",
        "    results = []\n",
        "    total_pis = len(pi_names)\n",
        "\n",
        "    for i, pi_name in enumerate(pi_names):\n",
        "        print(f\"Processing {i+1}/{total_pis}: {pi_name}\")\n",
        "\n",
        "        # Search ORCID\n",
        "        orcid_id = search_orcid_by_name(pi_name)\n",
        "        sleep(1.2)  # Rate limiting\n",
        "\n",
        "        if not orcid_id:\n",
        "            # Only include PIs with no ORCID if we're tracking them\n",
        "            continue\n",
        "\n",
        "        # Get works\n",
        "        works_data = get_orcid_works(orcid_id)\n",
        "        sleep(1.2)  # Rate limiting\n",
        "\n",
        "        publications = process_works(pi_name, orcid_id, works_data) if works_data else []\n",
        "\n",
        "        if publications:\n",
        "            results.extend(publications)\n",
        "\n",
        "    # Save all results\n",
        "    if results:\n",
        "        # Include journal in the output\n",
        "        pd.DataFrame(results)[['PI', 'year', 'title', 'DOI', 'journal', 'ORCID_ID']] \\\n",
        "          .to_csv(output_file, index=False)\n",
        "        print(f\"\\nDone! Saved {len(results)} FSRDC-related records to {output_file}\")\n",
        "    else:\n",
        "        print(\"\\nNo FSRDC-related publications found\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZe9X-4gaS08"
      },
      "source": [
        "CORE API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmlI5rF_TY0E"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "CORE_API_URL = \"https://api.core.ac.uk/v3/search/works\"\n",
        "CORE_API_KEY = \"JitbKoV5gnCsEOXGWywqeA9zdNcvZSul\"  # My API key\n",
        "INPUT_FILE = 'unique_pis.csv'\n",
        "OUTPUT_FILE = 'core_api_output.csv'\n",
        "CHECKPOINT_FILE = 'processing_checkpoint.txt'\n",
        "LOG_FILE = 'api_processing.log'\n",
        "\n",
        "# Rate limiting - 100 requests per minute = 0.6 seconds between requests\n",
        "REQUEST_DELAY = 0.6  # 600ms delay between API calls\n",
        "MAX_RETRIES = 3  # Max retries for failed requests\n",
        "RETRY_DELAY = 5  # Initial retry delay in seconds\n",
        "\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {CORE_API_KEY}',\n",
        "    'Accept': 'application/json'\n",
        "}\n",
        "\n",
        "# FSRDC keywords (case-insensitive)\n",
        "fsrdc_keywords = [\n",
        "    \"census bureau\", \"fsrdc\", \"federal statistical research data center\",\n",
        "    \"restricted microdata\", \"irs\", \"bea\", \"confidentiality review\",\n",
        "    \"michigan rdc\", \"texas rdc\", \"boston rdc\",\n",
        "    \"annual survey of manufactures\", \"census of construction industries\",\n",
        "    \"census of finance, insurance, and real estate\"\n",
        "]\n",
        "\n",
        "def log_message(message):\n",
        "    \"\"\"Log messages with timestamps\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(f\"[{timestamp}] {message}\\n\")\n",
        "    print(message)\n",
        "\n",
        "def save_checkpoint(index, pi_name):\n",
        "    \"\"\"Save current progress\"\"\"\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        f.write(f\"{index},{pi_name}\")\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load last checkpoint if exists\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            content = f.read().strip().split(',')\n",
        "            return int(content[0]), content[1]\n",
        "    return 0, \"\"\n",
        "\n",
        "def safe_lower(text):\n",
        "    \"\"\"Safely convert text to lowercase\"\"\"\n",
        "    return text.lower() if text else \"\"\n",
        "\n",
        "def is_fsrdc_related(work):\n",
        "    \"\"\"Check if paper is FSRDC-related\"\"\"\n",
        "    if not work or not isinstance(work, dict):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Combine all relevant text fields\n",
        "        text_fields = [\n",
        "            safe_lower(work.get('title', '')),\n",
        "            safe_lower(work.get('abstract', '')),\n",
        "            safe_lower(work.get('fullText', '')),\n",
        "            ' '.join([safe_lower(kw) for kw in work.get('keywords', []) if kw]),\n",
        "            ' '.join([safe_lower(inst.get('name', '')) for inst in work.get('institutions', []) if inst])\n",
        "        ]\n",
        "        combined_text = ' '.join(text_fields)\n",
        "        return any(keyword in combined_text for keyword in fsrdc_keywords)\n",
        "    except Exception as e:\n",
        "        log_message(f\"Error checking FSRDC relevance: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def make_api_request(url, params, retry_count=0):\n",
        "    \"\"\"Make API request with rate limit handling\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "        # Handle rate limiting\n",
        "        if response.status_code == 429:\n",
        "            retry_after = int(response.headers.get('Retry-After', 60))\n",
        "            log_message(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
        "            time.sleep(retry_after)\n",
        "            return make_api_request(url, params)  # Retry after waiting\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
        "\n",
        "        return response.json()\n",
        "\n",
        "    except Exception as e:\n",
        "        if retry_count < MAX_RETRIES:\n",
        "            wait_time = RETRY_DELAY * (retry_count + 1)\n",
        "            log_message(f\"Attempt {retry_count+1} failed. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "            return make_api_request(url, params, retry_count + 1)\n",
        "        else:\n",
        "            log_message(f\"Max retries exceeded for request: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def search_core_by_author(author_name):\n",
        "    \"\"\"Search CORE for works by author with pagination\"\"\"\n",
        "    if not author_name:\n",
        "        return []\n",
        "\n",
        "    params = {'q': f'authors:\"{author_name}\"', 'limit': 100, 'offset': 0}\n",
        "    all_results = []\n",
        "\n",
        "    while True:\n",
        "        data = make_api_request(CORE_API_URL, params)\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        results = data.get('results', [])\n",
        "        fsrdc_results = [r for r in results if is_fsrdc_related(r)]\n",
        "        all_results.extend(fsrdc_results)\n",
        "\n",
        "        if len(results) < params['limit']:\n",
        "            break\n",
        "\n",
        "        params['offset'] += params['limit']\n",
        "        time.sleep(REQUEST_DELAY)  # Maintain rate limit between paginated requests\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def process_core_results(pi_name, core_data):\n",
        "    \"\"\"Process API results into standardized format\"\"\"\n",
        "    records = []\n",
        "    for work in core_data or []:\n",
        "        try:\n",
        "            # Extract publication details\n",
        "            title = work.get('title', 'No title')\n",
        "            year = work.get('publishedDate', '')[:4] if work.get('publishedDate') else ''\n",
        "            doi = work.get('doi', '')\n",
        "\n",
        "            # Extract institutions\n",
        "            institutions = ', '.join(filter(None, [\n",
        "                inst.get('name', '') for inst in work.get('institutions', []) if inst\n",
        "            ]))\n",
        "\n",
        "            # Extract matched keyword\n",
        "            matched_keyword = next(\n",
        "                (kw for kw in fsrdc_keywords\n",
        "                 if kw in safe_lower(work.get('title', '') + ' ' + safe_lower(work.get('abstract', '')))),\n",
        "                ''\n",
        "            )\n",
        "\n",
        "            records.append({\n",
        "                'PI': pi_name,\n",
        "                'year': year,\n",
        "                'title': title,\n",
        "                'DOI': doi,\n",
        "                'Institutions': institutions,\n",
        "                'Matched_FSRDC_Keyword': matched_keyword,\n",
        "                'Keywords': ', '.join(work.get('keywords', [])),\n",
        "                'Source': 'CORE'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            log_message(f\"Error processing work for {pi_name}: {str(e)}\")\n",
        "\n",
        "    return records\n",
        "\n",
        "def main():\n",
        "    # Verify API key\n",
        "    test_response = make_api_request(CORE_API_URL, {'q': 'test', 'limit': 1})\n",
        "    if not test_response:\n",
        "        log_message(\"API connection test failed. Please check your API key.\")\n",
        "        return\n",
        "\n",
        "    # Load PI names\n",
        "    try:\n",
        "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader, None)  # Skip header\n",
        "            all_pi_names = [row[0].strip() for row in reader if row and row[0].strip()]\n",
        "    except Exception as e:\n",
        "        log_message(f\"Error reading input file: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Load or create results file\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        results_df = pd.read_csv(OUTPUT_FILE)\n",
        "    else:\n",
        "        results_df = pd.DataFrame(columns=[\n",
        "            'PI', 'year', 'title', 'DOI', 'Institutions',\n",
        "            'Matched_FSRDC_Keyword', 'Keywords', 'Source'\n",
        "        ])\n",
        "\n",
        "    # Resume from checkpoint\n",
        "    start_idx, last_pi = load_checkpoint()\n",
        "    if last_pi:\n",
        "        log_message(f\"Resuming from {last_pi} (index {start_idx})\")\n",
        "\n",
        "    # Process PIs\n",
        "    for i, pi_name in enumerate(all_pi_names[start_idx:], start=start_idx):\n",
        "        log_message(f\"\\nProcessing {i+1}/{len(all_pi_names)}: {pi_name}\")\n",
        "\n",
        "        try:\n",
        "            # Search and process results\n",
        "            core_data = search_core_by_author(pi_name)\n",
        "            publications = process_core_results(pi_name, core_data)\n",
        "\n",
        "            if publications:\n",
        "                new_data = pd.DataFrame(publications)\n",
        "                results_df = pd.concat([results_df, new_data], ignore_index=True)\n",
        "                log_message(f\"Found {len(publications)} FSRDC-related publications\")\n",
        "            else:\n",
        "                empty_record = pd.DataFrame([{\n",
        "                    'PI': pi_name,\n",
        "                    'year': '', 'title': '', 'DOI': '',\n",
        "                    'Institutions': '', 'Matched_FSRDC_Keyword': '',\n",
        "                    'Keywords': '', 'Source': 'CORE'\n",
        "                }])\n",
        "                results_df = pd.concat([results_df, empty_record], ignore_index=True)\n",
        "                log_message(\"No FSRDC-related publications found\")\n",
        "\n",
        "            # Save progress after each PI\n",
        "            save_checkpoint(i + 1, pi_name)\n",
        "            results_df.to_csv(OUTPUT_FILE, index=False)\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            log_message(\"\\nProcessing interrupted by user. Progress saved.\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            log_message(f\"Error processing {pi_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Clean up after completion\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        os.remove(CHECKPOINT_FILE)\n",
        "    log_message(\"\\nProcessing complete! Final results saved.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C16ACzQ_kq8z"
      },
      "source": [
        "Open Alex API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcnqvFook0dk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Step 1: Load the Excel file from GitHub\n",
        "url = \"https://github.com/dingkaihua/fsrdc-external-census-projects/raw/master/ProjectsAllMetadata.xlsx\"\n",
        "xls = pd.ExcelFile(url)\n",
        "\n",
        "# Step 2: Read the 'All Metadata' sheet\n",
        "df_metadata = xls.parse('All Metadata')\n",
        "\n",
        "# Step 3: Extract unique PI names\n",
        "pi_names = df_metadata['PI'].dropna().unique()\n",
        "\n",
        "print(\"First few PI names:\", pi_names[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ytszSQPlGY-"
      },
      "outputs": [],
      "source": [
        "#Storing the OpenAlex Ids of the PIs\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# OpenAlex API Endpoints\n",
        "openalex_author_url = \"https://api.openalex.org/authors\"\n",
        "\n",
        "# Dictionary to store PI -> OpenAlex ID mappings\n",
        "pi_openalex_ids = {}\n",
        "\n",
        "# Find OpenAlex ID for each PI\n",
        "for pi in pi_names:\n",
        "    query_name = pi.strip().lower()  # Normalize names\n",
        "    params = {\"search\": query_name}\n",
        "\n",
        "    response = requests.get(openalex_author_url, params=params)\n",
        "    time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data.get('results'):\n",
        "            # Pick the first matching author\n",
        "            author_info = data['results'][0]\n",
        "            pi_openalex_ids[pi] = author_info['id']\n",
        "        else:\n",
        "            pi_openalex_ids[pi] = \"Not Found\"\n",
        "    else:\n",
        "        pi_openalex_ids[pi] = \"API Error\"\n",
        "\n",
        "# Convert dictionary to DataFrame and save to CSV\n",
        "df = pd.DataFrame(list(pi_openalex_ids.items()), columns=[\"PI Name\", \"OpenAlex ID\"])\n",
        "df.to_csv(\"pi_openalex_ids.csv\", index=False)\n",
        "\n",
        "# Final print statement\n",
        "print(f\"\\n✅ Total PIs stored: {len(pi_openalex_ids)}\")\n",
        "print(\"📁 PI OpenAlex IDs saved to 'pi_openalex_ids.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZbGyiXzlimW"
      },
      "outputs": [],
      "source": [
        "#Filtering the research papers of the PIs using FSRDC keywords\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# OpenAlex API Endpoints\n",
        "openalex_works_url = \"https://api.openalex.org/works\"\n",
        "\n",
        "# Load PI OpenAlex IDs from CSV\n",
        "df_pis = pd.read_csv(\"pi_openalex_ids.csv\")\n",
        "pi_openalex_ids = dict(zip(df_pis[\"PI Name\"], df_pis[\"OpenAlex ID\"]))\n",
        "\n",
        "# Criteria for filtering research papers\n",
        "fsrdc_keywords = [\n",
        "    \"Census Bureau\", \"FSRDC\", \"Federal Statistical Research Data Center\",\n",
        "    \"restricted microdata\", \"IRS\", \"BEA\", \"confidentiality review\",\n",
        "    \"Michigan RDC\", \"Texas RDC\", \"Boston RDC\",\n",
        "    \"Annual Survey of Manufactures\", \"Census of Construction Industries\",\n",
        "    \"Census of Finance, Insurance, and Real Estate\"\n",
        "]\n",
        "\n",
        "fsrdc_keywords = [kw.lower() for kw in fsrdc_keywords]  # Convert to lowercase\n",
        "\n",
        "filtered_papers = []\n",
        "total_pis_processed = 0\n",
        "\n",
        "def fetch_papers(pi, author_id):\n",
        "    \"\"\"Fetch research papers for a given PI asynchronously.\"\"\"\n",
        "    if author_id in [\"Not Found\", \"API Error\"]:\n",
        "        return []\n",
        "\n",
        "    papers = []\n",
        "    page = 1\n",
        "    has_more_pages = True\n",
        "\n",
        "    while has_more_pages:\n",
        "        params = {\n",
        "            \"filter\": f\"authorships.author.id:{author_id}\",\n",
        "            \"per-page\": 200,\n",
        "            \"page\": page\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(openalex_works_url, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                works_data = response.json()\n",
        "                results = works_data.get('results', [])\n",
        "\n",
        "                for work in results:\n",
        "                    title = work.get('title', 'No Title')\n",
        "                    abstract = work.get('abstract_inverted_index', {})\n",
        "                    abstract_text = \" \".join(abstract.keys()).lower() if abstract else \"\"\n",
        "\n",
        "                    # Check for keyword matches\n",
        "                    title_lower = title.lower()\n",
        "                    if any(keyword in title_lower for keyword in fsrdc_keywords) or \\\n",
        "                       any(keyword in abstract_text for keyword in fsrdc_keywords):\n",
        "\n",
        "                        papers.append({\n",
        "                            \"PI\": pi,\n",
        "                            \"ORCID\": author_id,\n",
        "                            \"Paper Title\": title,\n",
        "                            \"Year\": work.get('publication_year', 'Unknown'),\n",
        "                            \"DOI\": work.get('doi', 'No DOI')\n",
        "                        })\n",
        "\n",
        "                has_more_pages = len(results) >= 50\n",
        "                page += 1\n",
        "\n",
        "            elif response.status_code == 429:  # Too many requests (API Rate Limit)\n",
        "                time.sleep(2)  # Wait and retry\n",
        "            else:\n",
        "                break  # Stop processing on error\n",
        "\n",
        "        except Exception:\n",
        "            break\n",
        "\n",
        "    return papers\n",
        "\n",
        "\n",
        "# Run API calls in parallel using ThreadPoolExecutor\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:  # 10 concurrent threads\n",
        "    future_to_pi = {executor.submit(fetch_papers, pi, author_id): pi for pi, author_id in pi_openalex_ids.items()}\n",
        "\n",
        "    for future in as_completed(future_to_pi):\n",
        "        pi = future_to_pi[future]\n",
        "        try:\n",
        "            papers = future.result()\n",
        "            filtered_papers.extend(papers)\n",
        "        except Exception:\n",
        "            pass  # Ignore errors for now\n",
        "\n",
        "        total_pis_processed += 1\n",
        "\n",
        "print(f\"\\n🔹 Processing complete!\")\n",
        "print(f\"   ✅ Total PIs processed: {total_pis_processed}\")\n",
        "print(f\"   ✅ Total matched papers: {len(filtered_papers)}\")\n",
        "\n",
        "def save_to_csv(results: List[Dict], filename: str):\n",
        "    \"\"\"Save results to CSV file\"\"\"\n",
        "    if not results:\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df = df[[\"PI\", \"ORCID\", \"Paper Title\", \"Year\", \"DOI\"]]\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "\n",
        "# Save to CSV\n",
        "save_to_csv(filtered_papers, \"openalex_api_output.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AERQ0GvLl3Pt"
      },
      "source": [
        "NIH PMC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjJ-bqKl7mE"
      },
      "outputs": [],
      "source": [
        "#Storing the PMC Ids of the PIs\n",
        "import time\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "import pandas as pd\n",
        "\n",
        "# NIH E-utilities API endpoints\n",
        "ncbi_esearch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "ncbi_efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "\n",
        "# Dictionary to store PI -> PubMed Central (PMC) IDs\n",
        "pi_pmc_ids = {}\n",
        "\n",
        "# Find PMC ID for each PI\n",
        "for pi in pi_names:\n",
        "    query_name = pi.strip()  # Normalize names\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",  # Searching PubMed (PMC articles are indexed here)\n",
        "        \"term\": query_name,\n",
        "        \"retmax\": 1,  # Limit to one result per query for now\n",
        "        \"retmode\": \"xml\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(ncbi_esearch_url, params=params)\n",
        "    time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parse the XML response\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "        id_list = root.find(\"IdList\")\n",
        "        if id_list is not None and len(id_list.findall(\"Id\")) > 0:\n",
        "            # Retrieve the first PubMed ID for the author\n",
        "            pmid = id_list.find(\"Id\").text\n",
        "            pi_pmc_ids[pi] = pmid\n",
        "    else:\n",
        "        pass  # Handle errors silently if needed\n",
        "\n",
        "# Convert dictionary to DataFrame and save to CSV\n",
        "df = pd.DataFrame(list(pi_pmc_ids.items()), columns=[\"PI Name\", \"PMC ID\"])\n",
        "df.to_csv(\"pi_pmc_ids.csv\", index=False)\n",
        "\n",
        "# Final print statement\n",
        "print(f\"\\nTotal PIs stored: {len(pi_pmc_ids)}\")\n",
        "print(\"PI PMC IDs saved to 'pi_pmc_ids.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2isLmklxno2Y"
      },
      "outputs": [],
      "source": [
        "#Filtering the research papers of the PIs using FSRDC keywords\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# NIH E-utilities API endpoints\n",
        "ncbi_esearch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "ncbi_efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "\n",
        "# Criteria for filtering research papers\n",
        "fsrdc_keywords = [\n",
        "    \"Census Bureau\", \"FSRDC\", \"Federal Statistical Research Data Center\",\n",
        "    \"restricted microdata\", \"IRS\", \"BEA\", \"confidentiality review\",\n",
        "    \"Michigan RDC\", \"Texas RDC\", \"Boston RDC\",\n",
        "    \"Annual Survey of Manufactures\", \"Census of Construction Industries\",\n",
        "    \"Census of Finance, Insurance, and Real Estate\"\n",
        "]\n",
        "fsrdc_keywords = [kw.lower() for kw in fsrdc_keywords]\n",
        "\n",
        "# Load PIs and their PMC IDs from the CSV file\n",
        "pi_data = pd.read_csv(\"pi_pmc_ids.csv\")\n",
        "pi_pmc_ids = dict(zip(pi_data[\"PI Name\"], pi_data[\"PMC ID\"]))\n",
        "\n",
        "filtered_papers = []\n",
        "total_pis_processed = 0\n",
        "\n",
        "# Process research papers for each PI\n",
        "for pi, author_id in pi_pmc_ids.items():\n",
        "    if not author_id:\n",
        "        continue\n",
        "\n",
        "    page = 1\n",
        "    has_more_pages = True\n",
        "\n",
        "    while has_more_pages:\n",
        "        params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"term\": author_id,\n",
        "            \"retmax\": 200,\n",
        "            \"retstart\": (page - 1) * 200,\n",
        "            \"retmode\": \"xml\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(ncbi_esearch_url, params=params)\n",
        "            time.sleep(1)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                root = ElementTree.fromstring(response.content)\n",
        "                id_list = root.find(\"IdList\")\n",
        "                if id_list is not None:\n",
        "                    pmids = [id_elem.text for id_elem in id_list.findall(\"Id\")]\n",
        "\n",
        "                    for pmid in pmids:\n",
        "                        efetch_params = {\n",
        "                            \"db\": \"pubmed\",\n",
        "                            \"id\": pmid,\n",
        "                            \"retmode\": \"xml\"\n",
        "                        }\n",
        "                        efetch_response = requests.get(ncbi_efetch_url, params=efetch_params)\n",
        "                        time.sleep(1)\n",
        "\n",
        "                        if efetch_response.status_code == 200:\n",
        "                            efetch_root = ElementTree.fromstring(efetch_response.content)\n",
        "                            article = efetch_root.find(\".//PubmedArticle\")\n",
        "\n",
        "                            if article is not None:\n",
        "                                title_elem = article.find(\".//ArticleTitle\")\n",
        "                                abstract_elem = article.find(\".//Abstract/AbstractText\")\n",
        "                                title = title_elem.text if title_elem is not None else 'No Title'\n",
        "                                abstract = abstract_elem.text if abstract_elem is not None else ''\n",
        "\n",
        "                                title_lower = title.lower()\n",
        "                                abstract_lower = abstract.lower()\n",
        "                                if (any(keyword in title_lower for keyword in fsrdc_keywords) or\n",
        "                                    any(keyword in abstract_lower for keyword in fsrdc_keywords)):\n",
        "\n",
        "                                    filtered_papers.append({\n",
        "                                        \"PI\": pi,\n",
        "                                        \"Year\": article.find(\".//PubDate/Year\").text if article.find(\".//PubDate/Year\") is not None else 'Unknown',\n",
        "                                        \"Title\": title,\n",
        "                                        \"DOI\": article.find(\".//ELocationID[@EIdType='doi']\").text if article.find(\".//ELocationID[@EIdType='doi']\") is not None else 'No DOI',\n",
        "                                        \"PMC ID\": pmid\n",
        "                                    })\n",
        "\n",
        "                if len(pmids) < 200:\n",
        "                    has_more_pages = False\n",
        "                else:\n",
        "                    page += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        except Exception:\n",
        "            break\n",
        "\n",
        "    total_pis_processed += 1\n",
        "\n",
        "# Final output\n",
        "print(f\"\\n🔹 Processing complete!\")\n",
        "print(f\"  Total PIs processed: {total_pis_processed}\")\n",
        "print(f\"  Total matched papers: {len(filtered_papers)}\")\n",
        "\n",
        "df = pd.DataFrame(filtered_papers)\n",
        "df.to_csv(\"pmc_api_ouput.csv\", index=False)\n",
        "print(f\"Results saved to 'pmc_api_ouput.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw0va8alxbNK"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6LBBkeWx7Kf"
      },
      "outputs": [],
      "source": [
        "# csv paths\n",
        "csv_files = [\n",
        "    'filtered_papers_openalex.csv',\n",
        "    'filtered_papers_pmc.csv',\n",
        "    'fsrdc_related_papers (1).csv',\n",
        "    'pi_publications_fsrdc_coreapi (1).csv',\n",
        "    'pi_publications_fsrdc_coreapi.csv',\n",
        "    'fsrdc_keyword_matches.csv'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_26fxoihz-GC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# add files\n",
        "\n",
        "# reformat all csvs\n",
        "target_cols = ['pi', 'title', 'year', 'doi']\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Normalize column names to lowercase\n",
        "    df.columns = df.columns.str.lower()\n",
        "\n",
        "    # Filter columns only if they exist\n",
        "    available_cols = [col for col in target_cols if col in df.columns]\n",
        "    filtered_df = df[available_cols]\n",
        "\n",
        "    dfs.append(filtered_df)\n",
        "\n",
        "# combine all dfs into one & save\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "print(len(combined_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQKzeVLDz7HO"
      },
      "outputs": [],
      "source": [
        "# reformat year + get rid of invalid dois + filter duplicates\n",
        "combined_df['year'] = pd.to_numeric(combined_df['year'], errors='coerce').astype('Int64')\n",
        "combined_df = combined_df.dropna(subset=['year'])\n",
        "combined_df = combined_df[combined_df['doi'].str.contains(r'^10\\.\\d{4,9}/.+', na=False)]\n",
        "combined_df = combined_df.drop_duplicates(subset='doi').copy()\n",
        "combined_df.to_csv('combined_output.csv', index=False)\n",
        "combined_df = combined_df[combined_df['year'] >= 1990]\n",
        "print(len(combined_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bhch1UY1B3Y"
      },
      "outputs": [],
      "source": [
        "# normalise data here\n",
        "combined_df['title_clean'] = combined_df['title'].str.lower().str.strip()\n",
        "\n",
        "# normalise data in the 2024 file\n",
        "other_df = pd.read_excel('ResearchOutputs (1).xlsx')\n",
        "other_df['title_clean'] = other_df['OutputTitle'].str.lower().str.strip()\n",
        "\n",
        "# compare\n",
        "combined_df = combined_df[~combined_df['title_clean'].isin(other_df['title_clean'])]\n",
        "combined_df.to_csv('unique_entries.csv', index=False) # final unique\n",
        "print(len(combined_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9SMURgx4Cdm"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzywuzzy[speedup]\n",
        "!pip install rapidfuzz\n",
        "!pip install rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jn8vsMz4LFq"
      },
      "outputs": [],
      "source": [
        "# additionally erase roughly similar\n",
        "from rapidfuzz import fuzz, process\n",
        "from rapidfuzz.distance import Levenshtein\n",
        "import numpy as np\n",
        "\n",
        "threshold = 90 # random similarity threshold\n",
        "\n",
        "# cleaned titles from excel to list\n",
        "titles_1 = combined_df['title_clean'].tolist()\n",
        "titles_2 = other_df['title_clean'].tolist()\n",
        "\n",
        "score_matrix = process.cdist(titles_1, titles_2, scorer=fuzz.token_sort_ratio)\n",
        "max_scores = np.max(score_matrix, axis=1)\n",
        "\n",
        "# show top 20 similarity scores\n",
        "print(max_scores[:20])\n",
        "print(\"Highest match score:\", max_scores.max())\n",
        "print(\"Average match score:\", max_scores.mean())\n",
        "\n",
        "combined_df['is_unique'] = max_scores < threshold\n",
        "unique_df = combined_df[combined_df['is_unique']]\n",
        "\n",
        "unique_df.to_csv('unique_entries.csv', index=False) # final unique\n",
        "print(len(unique_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTxq8XTy7l9I"
      },
      "source": [
        "# Graphing/Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60YnAAII7_8n"
      },
      "source": [
        "Make Data Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWhfvQmY7_if"
      },
      "outputs": [],
      "source": [
        "class Publication:\n",
        "  def __init__(self, title, year, pi, doi, keywords):\n",
        "     self.title = str(title)\n",
        "     self.year = self.safe_int(year)\n",
        "     self.pi = str(pi)\n",
        "     self.doi = str(doi)\n",
        "     self.keywords = str(keywords)\n",
        "\n",
        "  def safe_int(self, value):\n",
        "    try:\n",
        "        return int(value)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    # sort by title (alphabetically)\n",
        "    if self.title != other.title:\n",
        "        return self.title < other.title\n",
        "    return self.year < other.year\n",
        "\n",
        "  def __repr__(self):\n",
        "    # represent publication w title by default\n",
        "    return f\"Publication(title='{self.title}', year={self.year})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zVbaHyy9iCE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqmGqoTO9daQ"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT # import keybert for keyword extraction\n",
        "kw_model = KeyBERT() # initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuaFbgIn8Ia0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df2 = pd.read_csv('unique_entries.csv') # read the csv file\n",
        "\n",
        "df2[\"title\"] = df2[\"title\"].astype(str).fillna(\"\") # make sure all titles are strings + prevent the NaN exception\n",
        "for index, row in df2.iterrows(): # iterate through each row in the dataframe\n",
        "  title = row['title'] # get the title of the row from which we can extract keywords\n",
        "  keywords = kw_model.extract_keywords(title, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5) # extract top 5 keywords from the title\n",
        "  formatted_keywords = \"; \".join([kw[0] for kw in keywords]) # store the keywords as desired\n",
        "  print(formatted_keywords)\n",
        "  df2.loc[index, 'keywords'] = formatted_keywords # update cell\n",
        "\n",
        "df2[\"year\"] = df2[\"year\"].fillna(0).astype(int).replace(0, \"\") # makes my year into a number because omg\n",
        "df2.to_csv(\"data_with_keywords.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWlZfMk_CJC3"
      },
      "outputs": [],
      "source": [
        "publications = []\n",
        "for index, row in df2.iterrows():\n",
        "    publication = Publication(row['title'], row['year'], row['pi'], row['doi'], row['keywords'])\n",
        "    publications.append(publication)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtI-g_5Y78st"
      },
      "outputs": [],
      "source": [
        "import networkx as nx # import library for making graph\n",
        "\n",
        "G = nx.MultiGraph() # initialise graph\n",
        "G.add_nodes_from(publications[:20]) # add publications as nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmU2-44U_UG0"
      },
      "outputs": [],
      "source": [
        "edge_colours = {\n",
        "    \"title\": \"green\",\n",
        "    \"year\": \"red\",\n",
        "    \"pi\": \"blue\",\n",
        "    \"keyword\": \"purple\"\n",
        "}\n",
        "\n",
        "for publication in publications:\n",
        "  for another_publication in publications: # compare every publication with every publication\n",
        "    if publication is another_publication: # ignore if it's the same publication (doesn't need comparison])\n",
        "      continue\n",
        "    conditions = [ # check if title, year, pi or at least one keyword is alike\n",
        "      (publication.title == another_publication.title, \"title\"),\n",
        "      (publication.year == another_publication.year, \"year\"),\n",
        "      (publication.pi == another_publication.pi, \"pi\"),\n",
        "      (any(keyword in another_publication.keywords for keyword in publication.keywords.split(\"; \")), \"keyword\")\n",
        "    ]\n",
        "\n",
        "    for condition, colour_label in conditions:\n",
        "      if condition: # if title, year, agency or at least one keyword is alike, add edge\n",
        "        G.add_edge(publication, another_publication, colour=edge_colours[colour_label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj744p0Z_d2K"
      },
      "outputs": [],
      "source": [
        "# return connected components for a subgraph of a particular colour\n",
        "def count_colored_connected_components(G, colour):\n",
        "  # get connected nodes given colour\n",
        "  subgraph = G.edge_subgraph([(u, v, k) for u, v, k, data in G.edges(data=True, keys=True) if data.get(\"colour\") == colour]).copy()\n",
        "\n",
        "  # include isolated nodes\n",
        "  for node in G.nodes():\n",
        "    # make sure that we're not counting None as a separate component (though this is probably useless)\n",
        "    attribute_name = next((key for key, value in edge_colours.items() if value == colour), None)\n",
        "    if attribute_name and getattr(node, attribute_name, None) is not None:\n",
        "      if any(data.get(\"colour\") != colour for _, _, data in G.edges(node, data=True)):\n",
        "        subgraph.add_node(node)  # add isolated node\n",
        "\n",
        "\n",
        "  return len(list(nx.connected_components(subgraph)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7iThYCp_iWI"
      },
      "outputs": [],
      "source": [
        "total_nodes = len(G.nodes())\n",
        "total_edges = len(G.edges())\n",
        "edges_green = len([(u, v) for u, v, data in G.edges(data=True) if data.get(\"colour\") == \"green\"])\n",
        "edges_red = len([(u, v) for u, v, data in G.edges(data=True) if data.get(\"colour\") == \"red\"])\n",
        "edges_blue = len([(u, v) for u, v, data in G.edges(data=True) if data.get(\"colour\") == \"blue\"])\n",
        "edges_purple = len([(u, v) for u, v, data in G.edges(data=True) if data.get(\"colour\") == \"purple\"])\n",
        "number_connected_components_green = count_colored_connected_components(G, \"green\")\n",
        "number_connected_components_red = count_colored_connected_components(G, \"red\")\n",
        "number_connected_components_blue = count_colored_connected_components(G, \"blue\")\n",
        "number_connected_components_purple = count_colored_connected_components(G, \"purple\")\n",
        "\n",
        "\n",
        "print(f\"Total nodes: {total_nodes}\")\n",
        "print(f\"Total edges: {total_edges}\")\n",
        "print(f\"Edges with colour 'green': {edges_green}\")\n",
        "print(f\"Edges with colour 'red': {edges_red}\")\n",
        "print(f\"Edges with colour 'blue': {edges_blue}\")\n",
        "print(f\"Edges with colour 'purple': {edges_purple}\")\n",
        "print(f\"Number of connected components (green edges): {number_connected_components_green}\")\n",
        "print(f\"Number of connected components (red edges): {number_connected_components_red}\")\n",
        "print(f\"Number of connected components (blue edges): {number_connected_components_blue}\")\n",
        "print(f\"Number of connected components (purple edges): {number_connected_components_purple}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42d-MbS1_rfH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# sample a fixed number of nodes\n",
        "sample_size = 50\n",
        "selected_colour = \"red\"\n",
        "\n",
        "# sample random nodes for quick convenient plot\n",
        "sampled_nodes = random.sample(list(G.nodes()), min(len(G.nodes()), sample_size))\n",
        "\n",
        "# find all edges between the selected node that have the prespecified colour\n",
        "filtered_edges = [\n",
        "  (u, v) for u, v, data in G.edges(data=True)\n",
        "  if data.get(\"colour\") == selected_colour and u in sampled_nodes and v in sampled_nodes\n",
        "]\n",
        "\n",
        "# make a subgraph of these\n",
        "sampled_subgraph = nx.Graph()\n",
        "sampled_subgraph.add_nodes_from(sampled_nodes)\n",
        "sampled_subgraph.add_edges_from(filtered_edges)\n",
        "\n",
        "# add labels\n",
        "node_labels = {node: f\"{node.title[:20]}...\\n({node.year})\" for node in sampled_subgraph.nodes()} # cut title to make it prettier\n",
        "\n",
        "# plot the subgraph\n",
        "default_node_positions = nx.spring_layout(sampled_subgraph, k=0.5, seed=102)  # generate consistent layout, modify k to individual CCs closer together\n",
        "plt.figure(figsize=(10, 10))\n",
        "edge_colors = [G[u][v].get(\"colour\", \"black\") for u, v in sampled_subgraph.edges()]\n",
        "nx.draw(\n",
        "  sampled_subgraph,\n",
        "  pos=default_node_positions,\n",
        "  with_labels=True,\n",
        "  labels=node_labels,\n",
        "  node_size=100,\n",
        "  edge_color=selected_colour,\n",
        "  font_size=8\n",
        ")\n",
        "plt.title(f\"Subgraph of {sample_size} Random Nodes with {selected_colour.capitalize()} Edges\")\n",
        "plt.savefig(\"year_plot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50hri-6bOaZ2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# sample a fixed number of nodes\n",
        "sample_size = 5000\n",
        "selected_colour = \"green\"\n",
        "\n",
        "# sample random nodes for quick convenient plot\n",
        "sampled_nodes = random.sample(list(G.nodes()), min(len(G.nodes()), sample_size))\n",
        "\n",
        "# find all edges between the selected node that have the prespecified colour\n",
        "filtered_edges = [\n",
        "  (u, v) for u, v, data in G.edges(data=True)\n",
        "  if data.get(\"colour\") == selected_colour and u in sampled_nodes and v in sampled_nodes\n",
        "]\n",
        "\n",
        "# make a subgraph of these\n",
        "sampled_subgraph = nx.Graph()\n",
        "sampled_subgraph.add_nodes_from(sampled_nodes)\n",
        "sampled_subgraph.add_edges_from(filtered_edges)\n",
        "\n",
        "# add labels\n",
        "node_labels = {node: f\"{node.title[:20]}...\" for node in sampled_subgraph.nodes()} # cut title to make it prettier\n",
        "\n",
        "# plot the subgraph\n",
        "default_node_positions = nx.spring_layout(sampled_subgraph, k=0.5, seed=102)  # generate consistent layout, modify k to individual CCs closer together\n",
        "plt.figure(figsize=(10, 10))\n",
        "edge_colors = [G[u][v].get(\"colour\", \"black\") for u, v in sampled_subgraph.edges()]\n",
        "nx.draw(\n",
        "  sampled_subgraph,\n",
        "  pos=default_node_positions,\n",
        "  with_labels=True,\n",
        "  labels=node_labels,\n",
        "  node_size=100,\n",
        "  edge_color=selected_colour,\n",
        "  font_size=8\n",
        ")\n",
        "plt.title(f\"Subgraph of {sample_size} Random Nodes with {selected_colour.capitalize()} Edges\")\n",
        "plt.savefig(\"title_plot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcn6M0LGPq6d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# sample a fixed number of nodes\n",
        "sample_size = 100\n",
        "selected_colour = \"blue\"\n",
        "\n",
        "# sample random nodes for quick convenient plot\n",
        "sampled_nodes = random.sample(list(G.nodes()), min(len(G.nodes()), sample_size))\n",
        "\n",
        "# find all edges between the selected node that have the prespecified colour\n",
        "filtered_edges = [\n",
        "  (u, v) for u, v, data in G.edges(data=True)\n",
        "  if data.get(\"colour\") == selected_colour and u in sampled_nodes and v in sampled_nodes\n",
        "]\n",
        "\n",
        "# make a subgraph of these\n",
        "sampled_subgraph = nx.Graph()\n",
        "sampled_subgraph.add_nodes_from(sampled_nodes)\n",
        "sampled_subgraph.add_edges_from(filtered_edges)\n",
        "\n",
        "# add labels\n",
        "node_labels = {node: f\"{node.title[:20]}...\\n({node.pi})\" for node in sampled_subgraph.nodes()} # cut title to make it prettier\n",
        "\n",
        "# plot the subgraph\n",
        "default_node_positions = nx.spring_layout(sampled_subgraph, k=0.9, seed=102)  # generate consistent layout, modify k to individual CCs closer together\n",
        "plt.figure(figsize=(10, 10))\n",
        "edge_colors = [G[u][v].get(\"colour\", \"black\") for u, v in sampled_subgraph.edges()]\n",
        "nx.draw(\n",
        "  sampled_subgraph,\n",
        "  pos=default_node_positions,\n",
        "  with_labels=True,\n",
        "  labels=node_labels,\n",
        "  node_size=100,\n",
        "  edge_color=selected_colour,\n",
        "  font_size=8\n",
        ")\n",
        "plt.title(f\"Subgraph of {sample_size} Random Nodes with {selected_colour.capitalize()} Edges\")\n",
        "plt.savefig(\"pi_plot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq5QdUSoQbPg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# sample a fixed number of nodes\n",
        "sample_size = 150\n",
        "selected_colour = \"purple\"\n",
        "\n",
        "# sample random nodes for quick convenient plot\n",
        "sampled_nodes = random.sample(list(G.nodes()), min(len(G.nodes()), sample_size))\n",
        "\n",
        "# find all edges between the selected node that have the prespecified colour\n",
        "filtered_edges = [\n",
        "  (u, v) for u, v, data in G.edges(data=True)\n",
        "  if data.get(\"colour\") == selected_colour and u in sampled_nodes and v in sampled_nodes\n",
        "]\n",
        "\n",
        "# make a subgraph of these\n",
        "sampled_subgraph = nx.Graph()\n",
        "sampled_subgraph.add_nodes_from(sampled_nodes)\n",
        "sampled_subgraph.add_edges_from(filtered_edges)\n",
        "\n",
        "# add labels\n",
        "node_labels = {node: f\"{node.title[:20]}...\\n{node.year}\" for node in sampled_subgraph.nodes()} # cut title to make it prettier\n",
        "\n",
        "# plot the subgraph\n",
        "default_node_positions = nx.spring_layout(sampled_subgraph, k=0.5, seed=102)  # generate consistent layout, modify k to individual CCs closer together\n",
        "plt.figure(figsize=(10, 10))\n",
        "edge_colors = [G[u][v].get(\"colour\", \"black\") for u, v in sampled_subgraph.edges()]\n",
        "nx.draw(\n",
        "  sampled_subgraph,\n",
        "  pos=default_node_positions,\n",
        "  with_labels=True,\n",
        "  labels=node_labels,\n",
        "  node_size=100,\n",
        "  edge_color=selected_colour,\n",
        "  font_size=8\n",
        ")\n",
        "plt.title(f\"Subgraph of {sample_size} Random Nodes with {selected_colour.capitalize()} Edges\")\n",
        "plt.savefig(\"keywords_plot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data analysis and Visualization"
      ],
      "metadata": {
        "id": "xgUHktzZXvP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faz3cIj8R2G7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get num of projects per year\n",
        "projects_per_year = combined_df['year'].value_counts().sort_index()\n",
        "\n",
        "# plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "projects_per_year.plot(kind='bar')\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Projects\")\n",
        "plt.title(\"Number of Projects per Year\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"projects-over-time.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peak_year = papers_by_year.idxmax()\n",
        "peak_count = papers_by_year.max()\n",
        "print(f\"Peak year: {peak_year} ({peak_count} papers)\")"
      ],
      "metadata": {
        "id": "JzE6WTYxZuDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_2015 = papers_by_year[papers_by_year.index >= 2015].sum()\n",
        "pre_2015 = papers_by_year[papers_by_year.index < 2015].sum()\n",
        "growth = (post_2015 - pre_2015) / pre_2015 * 100\n",
        "print(f\"Post-2015 papers: {post_2015} | Growth: {growth:.1f}%\")"
      ],
      "metadata": {
        "id": "a27K8pfsZ6AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_avg = papers_by_year.rolling(window=3).mean()\n",
        "rolling_avg.plot(figsize=(12, 5), color='green', label='3-Year Avg')\n",
        "papers_by_year.plot(kind='line', alpha=0.3, label='Actual')\n",
        "plt.legend()\n",
        "plt.title(\"Smoothed Trend (3-Year Rolling Average)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eomahpcMZ-An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the csv file\n",
        "df = pd.read_csv(\"unique_entries.csv\")\n",
        "\n",
        "# Check first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check basic info (data types, non-null counts)\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "TrRblNfD4T9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Top publishing PIs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "top_pis= df.groupby('pi')['title'].count().sort_values(ascending=False).head(10)\n",
        "print(top_pis)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_pis.values, y=top_pis.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Publishing PIs\")\n",
        "plt.xlabel(\"Number of Publications\")\n",
        "plt.ylabel(\"Principal Investigator (PI)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2AHGrK-3R7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Create bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "X = vectorizer.fit_transform(df['title'].dropna().astype(str))\n",
        "bigrams = zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0])\n",
        "sorted_bigrams = sorted(bigrams, key=lambda x: x[1], reverse=True)[:15]\n",
        "\n",
        "# Plot\n",
        "bigram_labels, bigram_counts = zip(*sorted_bigrams)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(bigram_counts), y=list(bigram_labels), palette='crest')\n",
        "plt.title(\"Top Bigrams in Research Titles\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Bigram\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U5Gzk6fr3wQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Statistics & Visualization"
      ],
      "metadata": {
        "id": "KHuseSgZ4BnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:.0f}'.format  # No decimals\n",
        "print(df['year'].describe())"
      ],
      "metadata": {
        "id": "6dRIz5Qs4JgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique pis\n",
        "print(\"Unique pis:\", df['pi'].nunique())"
      ],
      "metadata": {
        "id": "pthbH5uU4MW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph showing frequency of publications over time.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['year'], bins=range(2010, 2024), edgecolor='black', alpha=0.7)\n",
        "plt.title(\"Distribution of FSRDC Papers by Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Papers\")\n",
        "plt.xticks(range(2010, 2024))\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pi0TwggO4OKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A box plot showing outliers in publication years.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.boxplot(df['year'].dropna(), vert=False, patch_artist=True)\n",
        "plt.title(\"Box Plot of Publication Years\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0eR4gkAy4PA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}